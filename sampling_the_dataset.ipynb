{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clone using 'git clone https://github.com/GoogleCloudPlatform/training-data-analyst '\n",
    "# change these to try this notebook out\n",
    "BUCKET = 'qwiklabs-gcp-00-79c1e4f2f7e0'\n",
    "PROJECT = 'cloud-training-demos'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating GCP bucket\n",
    "# %%bash\n",
    "# if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
    "#   gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "# fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SQL query using natality data after the year 2000\n",
    "from google.cloud import bigquery\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "  weight_pounds,\n",
    "  is_male,\n",
    "  mother_age,\n",
    "  plurality,\n",
    "  gestation_weeks,\n",
    "  ABS(FARM_FINGERPRINT(CONCAT(CAST(YEAR AS STRING), CAST(month AS STRING)))) AS hashmonth\n",
    "FROM\n",
    "  publicdata.samples.natality\n",
    "WHERE year > 2000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call BigQuery but GROUP BY the hashmonth and see number of records for each group to enable us to get the correct train and evaluation percentages\n",
    "df = bigquery.Client().query(\"SELECT hashmonth, COUNT(weight_pounds) AS num_babies FROM (\" + query + \") GROUP BY hashmonth\").to_dataframe()\n",
    "print(\"There are {} unique hashmonths.\".format(len(df)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added the RAND() so that we can now subsample from each of the hashmonths to get approximately the record counts we want\n",
    "##Train = 75%\n",
    "trainQuery = \"SELECT * FROM (\" + query + \") WHERE MOD(hashmonth, 4) < 3 AND RAND() < 0.0005\"\n",
    "evalQuery = \"SELECT * FROM (\" + query + \") WHERE MOD(hashmonth, 4) = 3 AND RAND() < 0.0005\"\n",
    "traindf = bigquery.Client().query(trainQuery).to_dataframe()\n",
    "evaldf = bigquery.Client().query(evalQuery).to_dataframe()\n",
    "print(\"There are {} examples in the train dataset and {} in the eval dataset\".format(len(traindf), len(evaldf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a small sample of the training data\n",
    "traindf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is always crucial to clean raw data before using in ML, so we have a preprocessing step\n",
    "import pandas as pd\n",
    "def preprocess(df):\n",
    "  # clean up data we don't want to train on\n",
    "  # in other words, users will have to tell us the mother's age\n",
    "  # otherwise, our ML service won't work.\n",
    "  # these were chosen because they are such good predictors\n",
    "  # and because these are easy enough to collect\n",
    "  df = df[df.weight_pounds > 0]\n",
    "  df = df[df.mother_age > 0]\n",
    "  df = df[df.gestation_weeks > 0]\n",
    "  df = df[df.plurality > 0]\n",
    "  \n",
    "  # modify plurality field to be a string\n",
    "  twins_etc = dict(zip([1,2,3,4,5],\n",
    "                   ['Single(1)', 'Twins(2)', 'Triplets(3)', 'Quadruplets(4)', 'Quintuplets(5)']))\n",
    "  df['plurality'].replace(twins_etc, inplace=True)\n",
    "  \n",
    "  # now create extra rows to simulate lack of ultrasound\n",
    "  nous = df.copy(deep=True)\n",
    "  nous.loc[nous['plurality'] != 'Single(1)', 'plurality'] = 'Multiple(2+)'\n",
    "  nous['is_male'] = 'Unknown'\n",
    "  \n",
    "  return pd.concat([df, nous])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.head()# Let's see a small sample of the training data now after our preprocessing\n",
    "traindf = preprocess(traindf)\n",
    "evaldf = preprocess(evaldf)\n",
    "traindf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe only does numeric columns, so you won't see plurality\n",
    "traindf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-9619bf2943f6>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-9619bf2943f6>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    In the final versions, we want to read from files, not Pandas dataframes. So, write the Pandas dataframes out as CSV files. Using CSV files gives us the advantage of shuffling during read. This is important for distributed training because some workers might be slower than others, and shuffling the data helps prevent the same data from being assigned to the slow workers.\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#In the final versions, we want to read from files, not Pandas dataframes. \n",
    "#So, write the Pandas dataframes out as CSV files. Using CSV files gives us the advantage\n",
    "#of shuffling during read. This is important for distributed training because some workers \n",
    "#might be slower than others, and shuffling the data helps prevent the same data from being assigned to the slow workers. \n",
    "\n",
    "traindf.to_csv('train.csv', index=False, header=False)\n",
    "evaldf.to_csv('eval.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wc -l *.csv\n",
    "head *.csv\n",
    "tail *.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
